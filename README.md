**Abuse-Detector-Agent**

![Python](https://img.shields.io/badge/Python-3.11-blue?logo=python&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-1.25-orange?logo=streamlit&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green)

-> **ğŸ›¡ï¸ ToxiGuard AI â€“ Abuse Detection System (AI Project)**
- *ğŸ§  Tech Stack: Python | Streamlit | NLP | Machine Learning | Plotly*

- âš¡ Real-time detection of abusive/toxic language with sentiment classification
- ğŸ¯ Confidence scoring with abusive keyword highlighting & severity analysis
- ğŸ“Š Interactive analytics dashboard for NLP insights
- ğŸ¨ Clean, scalable UI with custom CSS (production-ready)
- **ğŸ”— Repo:** [GitHub](https://github.com/wraith-klu/Abuse-Detector-Agent.git) | **ğŸŒ Live:** [Link](https://toxiguardagent.streamlit.app/)

- **Real-time AI-based abuse detector and text analysis tool** with sentiment insights, toxicity reporting, and word-level suggestions.

---

## ğŸš€ Features

- Detects abusive language in real-time text input.
- Highlights abusive words and provides polite replacement suggestions.
- Sentiment analysis (Positive, Neutral, Negative) with polarity scores.
- Toxicity gauge and severity distribution visualizations.
- Word frequency analysis and word cloud (non-abusive context).
- Interactive data table with sortable/filterable abusive words.
- Downloadable CSV report of abusive words and suggestions.
- History tracking of previous analyses.

---

## ğŸ› ï¸ Tech Stack

- **Backend:** Python, Streamlit
- **Machine Learning:** Scikit-learn, Joblib
- **NLP:** NLTK, TextBlob, Langdetect
- **Visualization:** Plotly, WordCloud, Matplotlib
- **Frontend:** Streamlit UI components, AgGrid

---

## ğŸ“‚ Repository Structure

```

Abuse-Detector-Agent/
â”œâ”€â”€ assets/                 # Images, background files, UI assets
â”œâ”€â”€ data/                   # Sample or training datasets
â”œâ”€â”€ utils/                  # Helper functions for preprocessing, cleaning, etc.
â”œâ”€â”€ abuse_model.joblib      # Trained ML model
â”œâ”€â”€ create_sample_data.py   # Script to generate sample data
â”œâ”€â”€ main.py                 # Main Streamlit app
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ train_model.py          # Script to train the abuse detection model

````

---

## ğŸ’» Installation

1. **Clone the repository**

```bash
git clone https://github.com/wraith-klu/Abuse-Detector-Agent.git
cd Abuse-Detector-Agent
````

2. **Create a virtual environment (optional but recommended)**

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

---

## ğŸƒâ€â™‚ï¸ Running the App

```bash
streamlit run main.py
```

* Open your browser at the URL shown in the terminal (usually `http://localhost:8501`)
* Enter text in the input box and click **Analyze Sentence** to see the predictions and insights.

---

## ğŸ“Š Usage Examples

* Detect abusive content in chat messages, comments, or social media text.
* Get sentiment insights and suggested replacements for harsh words.
* Visualize abusive word severity and frequency in interactive plots.

---

## âš™ï¸ Notes

* Make sure `abuse_model.joblib` is present in the root directory.
* Background images and assets are in the `assets/` folder.
* Stopwords are downloaded via NLTK automatically on first run.

---

## ğŸ“„ License

MIT License Â© 2025 Wraith-KLU

---

## ğŸ“ Contact

Developed by **Saurabh Yadav**
GitHub: [https://github.com/wraith-klu](https://github.com/wraith-klu)

